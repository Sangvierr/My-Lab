from airflow import DAG
from airflow.decorators import task
from airflow.models import Variable
from datetime import datetime

# 1. Airflow ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
WIN_IP = Variable.get("WIN_SERVER_IP")
MAC_IP = Variable.get("MAC_SERVER_IP",)
MODEL_NAME_VAR = Variable.get("OLLAMA_MODEL", default_var="qwen3:4b")

# 2. API ì£¼ì†Œ ì¡°í•©
OLLAMA_API_URL = f"http://{WIN_IP}:11434/api/generate"
JAVA_API_URL = f"http://{MAC_IP}:8090/api/news"

with DAG(
    dag_id="02_naver_news_scraper",
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
    tags=["mylab", "ai_pipeline"],
) as dag:

    # ---------------------------------------------------------
    # 1. ë‰´ìŠ¤ í¬ë¡¤ë§ (ë„¤ì´ë²„ IT ë­í‚¹ë‰´ìŠ¤)
    # ---------------------------------------------------------
    @task.virtualenv(
        task_id="scrape_naver_it_news",
        requirements=["requests", "beautifulsoup4"],
        system_site_packages=False
    )
    def scrape_news():
        import requests
        from bs4 import BeautifulSoup
        import time

        print("ğŸ“¡ [ì‹œì‘] ë„¤ì´ë²„ IT ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        url = "https://news.naver.com/main/ranking/popularDay.naver?mid=etc&sid1=105"
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        try:
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            news_boxes = soup.select('.rankingnews_box')
        except Exception as e:
            print(f"âŒ [ì—ëŸ¬] ë­í‚¹ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}")
            return []
        
        results = []
        
        if news_boxes:
            target_box = news_boxes[0]
            articles = target_box.select('li a')
            
            # ìƒìœ„ 3ê°œë§Œ ìˆ˜ì§‘
            for i, article in enumerate(articles[:1]): 
                title = article.text.strip()
                link = article['href']
                content = ""
                
                print(f"ğŸ“¡ [ìš”ì²­] ê¸°ì‚¬ ì ‘ì† ì¤‘: {title[:15]}...")
                
                try:
                    res = requests.get(link, headers=headers)
                    bs = BeautifulSoup(res.text, 'html.parser')
                    # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì˜ì—­
                    content_area = bs.select_one('#dic_area')
                    
                    if content_area:
                        content = content_area.get_text(strip=True)
                        print(f"âœ… [ìˆ˜ì§‘] ë³¸ë¬¸ í¬ë¡¤ë§ ì„±ê³µ ({len(content)}ì)")
                    else:
                        print(f"âŒ [ì‹¤íŒ¨] ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                except Exception as e:
                    print(f"âŒ [ì—ëŸ¬] ê¸°ì‚¬ ì ‘ì† ì‹¤íŒ¨: {e}")

                if content:
                    results.append({'rank': i+1, 'title': title, 'link': link, 'content': content})
                    time.sleep(0.5)
        else:
            print("âŒ [ì‹¤íŒ¨] ë­í‚¹ ë‰´ìŠ¤ ë°•ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        print(f"âœ… [ì™„ë£Œ] ì´ {len(results)}ê±´ì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return results

    # ---------------------------------------------------------
    # 2. AI ìš”ì•½ (Ollama)
    # ---------------------------------------------------------
    @task.virtualenv(
        task_id="summarize_with_ollama",
        requirements=["requests"],
        system_site_packages=False
    )
    def summarize_news(news_list: list, api_url: str, model_name: str):
        import requests
        import json

        print(f"ğŸ“¡ [ì‹œì‘] Ollama({model_name})ì—ê²Œ {len(news_list)}ê±´ì˜ ìš”ì•½ì„ ìš”ì²­í•©ë‹ˆë‹¤.")

        summarized_results = []
        for news in news_list:
            original_text = news['content']
            # 2000ì ì œí•œ
            if len(original_text) > 2000:
                original_text = original_text[:2000] + "..."

            prompt = f"""
            ì•„ë˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ '3ì¤„ ìš”ì•½' í˜•ì‹ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì¤˜.
            ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ìš”ì•½ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´.

            [ê¸°ì‚¬ ë³¸ë¬¸]
            {original_text}
            """

            payload = {
                "model": model_name,
                "prompt": prompt,
                "stream": False
            }

            try:
                print(f"ğŸ“¡ [ìš”ì²­] ìš”ì•½ ì§„í–‰ ì¤‘: {news['title'][:15]}...")
                response = requests.post(api_url, json=payload, timeout=120)
                
                if response.status_code == 200:
                    summary = response.json().get('response', 'ìš”ì•½ ì‹¤íŒ¨')
                    # ë¡œê·¸ ê¹”ë”í•˜ê²Œ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
                    clean_summary = summary.replace('\n', ' ')[:30]
                    print(f"âœ… [ì„±ê³µ] {clean_summary}...")
                    
                    news['summary'] = summary
                    del news['content']  # ì›ë³¸ ë³¸ë¬¸ ì œê±° (DB ìš©ëŸ‰ ì ˆì•½)
                    summarized_results.append(news)
                else:
                    print(f"âŒ [ì‹¤íŒ¨] ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                    print(f"   ã„´ ë‚´ìš©: {response.text}")

            except Exception as e:
                print(f"âŒ [ì—ëŸ¬] Ollama ì—°ê²° ì‹¤íŒ¨: {e}")

        print(f"âœ… [ì™„ë£Œ] ì´ {len(summarized_results)}ê±´ ìš”ì•½ ì™„ë£Œ")
        return summarized_results

    # ---------------------------------------------------------
    # 3. Java ë°±ì—”ë“œë¡œ ì „ì†¡
    # ---------------------------------------------------------
    @task.virtualenv(
        task_id="send_to_java_backend",
        requirements=["requests"],
        system_site_packages=False
    )
    def send_to_java(news_list: list, java_url: str):
        import requests
        import json

        print(f"ğŸ“¡ [ì‹œì‘] Java ì„œë²„({java_url})ë¡œ ë°ì´í„° ì „ì†¡ ì‹œì‘")

        success_count = 0
        for news in news_list:
            try:
                payload = {
                    "title": news['title'],
                    "link": news['link'],
                    "summary": news['summary']
                }
                
                headers = {'Content-Type': 'application/json'}
                print(f"ğŸ“¡ [ì „ì†¡] {news['title'][:15]}...")
                response = requests.post(java_url, json=payload, headers=headers, timeout=10)

                if response.status_code == 200:
                    print(f"âœ… [ì„±ê³µ] ì„œë²„ ì €ì¥ ì™„ë£Œ")
                    success_count += 1
                else:
                    print(f"âŒ [ì‹¤íŒ¨] ì„œë²„ ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            
            except Exception as e:
                print(f"âŒ [ì—ëŸ¬] Java ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")

        print(f"âœ… [ì™„ë£Œ] ì´ {success_count}/{len(news_list)} ê±´ ì „ì†¡ ì„±ê³µ")

    # ---------------------------------------------------------
    # 4. íŒŒì´í”„ë¼ì¸ ì—°ê²°
    # ---------------------------------------------------------
    raw_news = scrape_news()
    summarized_news = summarize_news(raw_news, api_url=OLLAMA_API_URL, model_name=MODEL_NAME_VAR)
    send_to_java(summarized_news, java_url=JAVA_API_URL)